#!/usr/bin/env node
/**
 * batchMigrateOnDemandSchemas.mjs
 *
 * Migrate (schemaâ€‘only) an array of empty, onâ€‘demand DynamoDB tables from
 * one AWS account to another. This version assumes ALL tables have
 * BillingMode = PAY_PER_REQUEST, so it omits any ProvisionedThroughput logic.
 *
 * Usage:
 *   node batchMigrateOnDemandSchemas.mjs \
 *     --source-profile sourceAccountProfile \
 *     --dest-profile destAccountProfile \
 *     [--region us-east-1]
 *
 * It will process these table names:
 *   - babies
 *   - mothers
 *   - reminders
 *   - vaccination_schedule
 */

import { DynamoDBClient,
         DescribeTableCommand,
         ListTagsOfResourceCommand,
         DescribeTimeToLiveCommand,
         DescribeContinuousBackupsCommand,
         CreateTableCommand,
         UpdateTimeToLiveCommand,
         UpdateContinuousBackupsCommand,
         waitUntilTableExists } from "@aws-sdk/client-dynamodb";

import { fromIni } from "@aws-sdk/credential-provider-ini";
import yargsPkg from "yargs";
import { hideBin } from "yargs/helpers";
import process from "process";

// â€”â€”â€” CLI Argument Parsing â€”â€”â€”
const argv = yargsPkg(hideBin(process.argv))
  .option("source-profile", { type: "string", demandOption: true })
  .option("dest-profile",   { type: "string", demandOption: true })
  .option("region",         { type: "string", default: "us-east-1" })
  .help()
  .alias("help", "h")
  .parse();

const SOURCE_PROFILE = argv["source-profile"];
const DEST_PROFILE   = argv["dest-profile"];
const REGION         = argv["region"];

// â€”â€”â€” List of tables to migrate â€”â€”â€”
const TABLES_TO_MIGRATE = [
  "babies",
  "mothers",
  "reminders",
  "vaccination_schedule",
];

// â€”â€”â€” Helper: Build schema from a source table â€”â€”â€”
async function getTableSchema(ddbClient, tableName) {
  const { Table } = await ddbClient.send(
    new DescribeTableCommand({ TableName: tableName })
  );
  if (!Table) {
    throw new Error(`Table "${tableName}" not found in source account.`);
  }

  // We know BillingMode is onâ€‘demand, so we hardcode PAY_PER_REQUEST.
  // We still copy KeySchema, AttributeDefinitions, indexes, streams, etc.
  const schema = {
    TableName: Table.TableName,
    AttributeDefinitions: Table.AttributeDefinitions,
    KeySchema: Table.KeySchema,
    BillingMode: "PAY_PER_REQUEST",
  };

  // 1) Global Secondary Indexes (if any)
  if (Array.isArray(Table.GlobalSecondaryIndexes)) {
    schema.GlobalSecondaryIndexes = Table.GlobalSecondaryIndexes.map((gsi) => ({
      IndexName:  gsi.IndexName,
      KeySchema:  gsi.KeySchema,
      Projection: gsi.Projection,
      // Onâ€‘demand GSIs donâ€™t need ProvisionedThroughput either
    }));
  }

  // 2) Local Secondary Indexes (if any)
  if (Array.isArray(Table.LocalSecondaryIndexes)) {
    schema.LocalSecondaryIndexes = Table.LocalSecondaryIndexes.map((lsi) => ({
      IndexName:  lsi.IndexName,
      KeySchema:  lsi.KeySchema,
      Projection: lsi.Projection,
    }));
  }

  // 3) StreamSpecification (if enabled)
  if (Table.StreamSpecification?.StreamEnabled) {
    schema.StreamSpecification = {
      StreamEnabled: true,
      StreamViewType: Table.StreamSpecification.StreamViewType,
    };
  }

  // 4) SSESpecification (if enabled)
  if (Table.SSEDescription?.Status === "ENABLED") {
    schema.SSESpecification = {
      Enabled: true,
      SSEType: Table.SSEDescription.SSEType, // usually "KMS"
      // If you used a custom CMK, add KMSMasterKeyId here.
      // KMSMasterKeyId: "<yourâ€‘kmsâ€‘keyâ€‘arn>"
    };
  }

  // 5) Tags (optional)
  const tagsResp = await ddbClient.send(
    new ListTagsOfResourceCommand({ ResourceArn: Table.TableArn })
  );
  if (Array.isArray(tagsResp.Tags) && tagsResp.Tags.length) {
    schema.Tags = tagsResp.Tags.map((t) => ({ Key: t.Key, Value: t.Value }));
  }

  // 6) TimeToLive (postâ€‘creation)
  const ttlResp = await ddbClient.send(
    new DescribeTimeToLiveCommand({ TableName: tableName })
  );
  const ttlDesc = ttlResp.TimeToLiveDescription;
  if (ttlDesc?.TimeToLiveStatus === "ENABLED") {
    schema._EnableTTL = {
      Enabled: true,
      AttributeName: ttlDesc.AttributeName,
    };
  }

  // 7) Pointâ€‘Inâ€‘Time Recovery (postâ€‘creation)
  const pitrResp = await ddbClient.send(
    new DescribeContinuousBackupsCommand({ TableName: tableName })
  );
  const pitrDesc = pitrResp.ContinuousBackupsDescription
    .PointInTimeRecoveryDescription;
  if (pitrDesc?.PointInTimeRecoveryStatus === "ENABLED") {
    schema._EnablePITR = true;
  }

  return schema;
}

// â€”â€”â€” Helper: Create table in destination + enable TTL/PITR â€”â€”â€”
async function createTableInDest(ddbClient, schema, destTableName) {
  const createParams = {
    TableName: destTableName,
    AttributeDefinitions: schema.AttributeDefinitions,
    KeySchema: schema.KeySchema,
    BillingMode: "PAY_PER_REQUEST",
  };

  if (Array.isArray(schema.GlobalSecondaryIndexes)) {
    createParams.GlobalSecondaryIndexes = schema.GlobalSecondaryIndexes;
  }
  if (Array.isArray(schema.LocalSecondaryIndexes)) {
    createParams.LocalSecondaryIndexes = schema.LocalSecondaryIndexes;
  }
  if (schema.StreamSpecification) {
    createParams.StreamSpecification = schema.StreamSpecification;
  }
  if (schema.SSESpecification) {
    createParams.SSESpecification = schema.SSESpecification;
  }
  if (Array.isArray(schema.Tags)) {
    createParams.Tags = schema.Tags;
  }

  console.log(`â†’ Creating table "${destTableName}" in destination account...`);
  try {
    await ddbClient.send(new CreateTableCommand(createParams));
  } catch (err) {
    console.error(`ERROR: Could not create "${destTableName}":`, err);
    process.exit(1);
  }

  console.log(`â†’ Waiting for "${destTableName}" to become ACTIVEâ€¦`);
  await waitUntilTableExists(
    { client: ddbClient, maxWaitTime: 300 },
    { TableName: destTableName }
  );
  console.log(`â†’ "${destTableName}" is now ACTIVE.`);

  // Enable TTL if needed
  if (schema._EnableTTL) {
    console.log(
      `â†’ Enabling TTL on "${destTableName}" (attribute="${schema._EnableTTL.AttributeName}")â€¦`
    );
    await ddbClient.send(
      new UpdateTimeToLiveCommand({
        TableName: destTableName,
        TimeToLiveSpecification: {
          Enabled: true,
          AttributeName: schema._EnableTTL.AttributeName,
        },
      })
    );
    console.log("â†’ TTL enabled.");
  }

  // Enable PITR if needed
  if (schema._EnablePITR) {
    console.log(`â†’ Enabling Pointâ€‘Inâ€‘Time Recovery on "${destTableName}"â€¦`);
    await ddbClient.send(
      new UpdateContinuousBackupsCommand({
        TableName: destTableName,
        PointInTimeRecoverySpecification: {
          PointInTimeRecoveryEnabled: true,
        },
      })
    );
    console.log("â†’ PITR enabled.");
  }

  console.log(`âœ… Schema for "${destTableName}" created successfully.`);
}

// â€”â€”â€” Main Flow â€”â€”â€”
async function main() {
  // 1) Initialize DynamoDB clients for source & destination
  const srcClient = new DynamoDBClient({
    region: REGION,
    credentials: fromIni({ profile: SOURCE_PROFILE }),
  });
  const dstClient = new DynamoDBClient({
    region: REGION,
    credentials: fromIni({ profile: DEST_PROFILE }),
  });

  for (const tableName of TABLES_TO_MIGRATE) {
    console.log(`\n=== Processing table: "${tableName}" ===`);

    // 2) Fetch schema from source
    console.log(`â†’ Fetching schema for "${tableName}" from source account...`);
    const schema = await getTableSchema(srcClient, tableName);

    // 3) Create table in destination
    await createTableInDest(dstClient, schema, tableName);
  }

  console.log("\nðŸŽ‰ All four tables have been migrated (schemaâ€‘only, PAY_PER_REQUEST).");
}

main().catch((err) => {
  console.error("Fatal error in batch migration:", err);
  process.exit(1);
});
